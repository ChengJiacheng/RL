{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 23:01:00 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 431.60       Driver Version: 431.60       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 207... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P8    11W /  N/A |    646MiB /  8192MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    136508    C+G   ...6)\\Google\\Chrome\\Application\\chrome.exe N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "LEAVE_PRINT_EVERY_N_SECS = 20\n",
    "ERASE_LINE = '\\x1b[2K'\n",
    "EPS = 1e-6\n",
    "BEEP = lambda: os.system(\"printf '\\a'\")\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_env_fn(**kargs):\n",
    "    def make_env_fn(env_name, seed=None, unwrapped=False, \n",
    "                    monitor_mode=None, addon_wrappers=None):\n",
    "        mdir = tempfile.mkdtemp()\n",
    "        env = gym.make(env_name)\n",
    "        if seed is not None: env.seed(seed)\n",
    "        env = env.unwrapped if unwrapped else env\n",
    "        env = wrappers.Monitor(\n",
    "            env, mdir, force=True, mode=monitor_mode) if monitor_mode else env\n",
    "        if addon_wrappers:\n",
    "            for wrapper in addon_wrappers:\n",
    "                env = wrapper(env)\n",
    "        return env\n",
    "    return make_env_fn, kargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, title, max_n_videos=5):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(env_videos, title, max_n_videos=5):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        basename = os.path.splitext(video_path)[0]\n",
    "        gif_path = basename + '.gif'\n",
    "        if not os.path.exists(gif_path):\n",
    "            ps = subprocess.Popen(\n",
    "                ('ffmpeg', \n",
    "                 '-i', video_path, \n",
    "                 '-r', '10', \n",
    "                 '-f', 'image2pipe', \n",
    "                 '-vcodec', 'ppm', \n",
    "                 '-'), \n",
    "                stdout=subprocess.PIPE)\n",
    "            output = subprocess.check_output(\n",
    "                ('convert', \n",
    "                 '-delay', '5', \n",
    "                 '-loop', '0', \n",
    "                 '-', gif_path), \n",
    "                stdin=ps.stdout)\n",
    "            ps.wait()\n",
    "\n",
    "        gif = io.open(gif_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(gif)\n",
    "            \n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscountedCartPole(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    def step(self, a):\n",
    "        o, r, d, _ = self.env.step(a)\n",
    "        (x, x_dot, theta, theta_dot) = o\n",
    "        pole_fell =  x < -self.env.unwrapped.x_threshold \\\n",
    "                    or x > self.env.unwrapped.x_threshold \\\n",
    "                    or theta < -self.env.unwrapped.theta_threshold_radians \\\n",
    "                    or theta > self.env.unwrapped.theta_threshold_radians\n",
    "        r = -1 if pole_fell else 0\n",
    "        return o, r, d, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, \n",
    "                                     hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\n",
    "                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(\n",
    "            hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        self.exploratory_action_taken = False\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyStrategy():\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFQ():\n",
    "    def __init__(self, \n",
    "                 value_model_fn, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr,\n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 batch_size,\n",
    "                 epochs):\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "        \n",
    "        max_a_q_sp = self.online_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        target_q_s = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_errors = q_sa - target_q_s\n",
    "        value_loss = td_errors.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        action = self.training_strategy.select_action(self.online_model, state)\n",
    "        new_state, reward, is_terminal, _ = env.step(action)\n",
    "        past_limit_enforced = hasattr(env, '_past_limit') and env._past_limit()\n",
    "        is_failure = is_terminal and not past_limit_enforced\n",
    "\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "        self.experiences.append(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.online_model = self.value_model_fn(nS, nA)\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n",
    "                                                       self.value_optimizer_lr)\n",
    "\n",
    "        self.training_strategy = training_strategy_fn()\n",
    "        self.evaluation_strategy = evaluation_strategy_fn() \n",
    "        self.experiences = []\n",
    "\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "                \n",
    "                if len(self.experiences) >= self.batch_size:\n",
    "                    experiences = np.array(self.experiences)\n",
    "                    batches = [np.vstack(sars) for sars in experiences.T]\n",
    "                    experiences = self.online_model.load(batches)\n",
    "                    for _ in range(self.epochs):\n",
    "                        self.optimize_model(experiences)\n",
    "                    self.experiences.clear()\n",
    "                \n",
    "                if is_terminal:\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_model, env)\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(self.online_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "    \n",
    "    def demo(self, title='Trained {} Agent', n_episodes=10, max_n_videos=3):          \n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation')\n",
    "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:02, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\n",
      "\u001b[2Kel 00:22, ep 0507, ts 012414, ar 10 079.0±033.4, 100 045.3±024.7, ex 100 0.3±0.1, ev 059.2±033.7\n",
      "\u001b[2Kel 00:42, ep 0843, ts 024319, ar 10 046.4±021.3, 100 036.1±016.4, ex 100 0.2±0.1, ev 038.0±005.8\n",
      "\u001b[2Kel 01:02, ep 0956, ts 034463, ar 10 122.8±059.4, 100 094.2±057.4, ex 100 0.3±0.1, ev 163.2±073.6\n",
      "\u001b[2Kel 01:23, ep 1168, ts 044412, ar 10 087.1±056.8, 100 035.7±044.1, ex 100 0.2±0.1, ev 055.3±070.6\n",
      "\u001b[2Kel 01:43, ep 1248, ts 054437, ar 10 130.0±062.4, 100 118.6±065.9, ex 100 0.3±0.1, ev 217.7±075.2\n",
      "\u001b[2Kel 02:03, ep 1324, ts 064066, ar 10 149.5±106.6, 100 124.8±077.7, ex 100 0.3±0.1, ev 259.6±104.9\n",
      "\u001b[2Kel 02:23, ep 1382, ts 072323, ar 10 059.5±053.1, 100 144.8±089.8, ex 100 0.3±0.1, ev 336.1±115.7\n",
      "\u001b[2Kel 02:43, ep 1468, ts 082016, ar 10 136.9±079.5, 100 109.3±073.7, ex 100 0.3±0.1, ev 222.1±085.7\n",
      "\u001b[2Kel 03:03, ep 1533, ts 091001, ar 10 102.3±067.9, 100 138.6±085.6, ex 100 0.3±0.1, ev 298.1±096.0\n",
      "\u001b[2Kel 03:23, ep 1585, ts 099188, ar 10 166.9±086.2, 100 148.6±099.4, ex 100 0.3±0.1, ev 362.0±106.6\n",
      "\u001b[2Kel 03:44, ep 1638, ts 107223, ar 10 256.6±140.1, 100 153.0±112.7, ex 100 0.3±0.1, ev 414.5±093.3\n",
      "\u001b[2Kel 04:04, ep 1693, ts 115706, ar 10 143.4±118.0, 100 156.7±112.4, ex 100 0.3±0.1, ev 403.7±112.4\n",
      "\u001b[2Kel 04:24, ep 1747, ts 124613, ar 10 209.9±100.6, 100 156.7±105.9, ex 100 0.3±0.1, ev 385.5±108.1\n",
      "\u001b[2Kel 04:44, ep 1797, ts 133393, ar 10 095.8±069.1, 100 171.1±119.4, ex 100 0.3±0.1, ev 409.2±098.7\n",
      "\u001b[2Kel 05:04, ep 1840, ts 142763, ar 10 241.9±148.5, 100 193.4±133.7, ex 100 0.3±0.1, ev 440.6±082.8\n",
      "\u001b[2Kel 05:24, ep 1897, ts 151287, ar 10 193.6±054.7, 100 178.9±124.2, ex 100 0.3±0.1, ev 409.4±106.8\n",
      "\u001b[2Kel 05:44, ep 1944, ts 159159, ar 10 150.6±073.0, 100 160.8±105.0, ex 100 0.3±0.1, ev 375.5±105.6\n",
      "\u001b[2Kel 06:04, ep 1991, ts 167331, ar 10 164.1±082.6, 100 172.2±113.9, ex 100 0.3±0.1, ev 377.4±093.1\n",
      "\u001b[2Kel 06:24, ep 2035, ts 175794, ar 10 227.1±093.7, 100 179.6±112.5, ex 100 0.3±0.1, ev 404.4±095.0\n",
      "\u001b[2Kel 06:45, ep 2076, ts 183248, ar 10 224.0±107.5, 100 179.0±109.9, ex 100 0.3±0.1, ev 426.6±089.1\n",
      "\u001b[2Kel 07:05, ep 2122, ts 190645, ar 10 185.4±130.3, 100 179.4±118.8, ex 100 0.3±0.1, ev 422.4±091.2\n",
      "\u001b[2Kel 07:25, ep 2168, ts 198176, ar 10 159.9±109.2, 100 166.8±124.5, ex 100 0.3±0.1, ev 415.4±082.6\n",
      "\u001b[2Kel 07:45, ep 2211, ts 205178, ar 10 209.3±117.9, 100 166.7±122.6, ex 100 0.3±0.1, ev 440.3±072.3\n",
      "\u001b[2Kel 08:05, ep 2258, ts 212670, ar 10 206.2±148.1, 100 160.9±113.4, ex 100 0.3±0.1, ev 433.7±074.9\n",
      "\u001b[2Kel 08:26, ep 2311, ts 220329, ar 10 136.0±072.0, 100 151.5±100.2, ex 100 0.3±0.1, ev 404.1±088.4\n",
      "\u001b[2Kel 08:46, ep 2361, ts 228086, ar 10 164.4±088.3, 100 149.0±101.1, ex 100 0.3±0.1, ev 417.6±097.2\n",
      "\u001b[2Kel 09:06, ep 2412, ts 236174, ar 10 143.3±090.7, 100 153.8±100.1, ex 100 0.3±0.1, ev 433.1±085.6\n",
      "\u001b[2Kel 09:26, ep 2462, ts 244233, ar 10 139.9±089.0, 100 159.0±111.8, ex 100 0.3±0.1, ev 408.2±088.7\n",
      "\u001b[2Kel 09:46, ep 2501, ts 250608, ar 10 245.2±135.8, 100 160.7±119.2, ex 100 0.3±0.1, ev 423.8±086.3\n",
      "\u001b[2Kel 10:06, ep 2541, ts 258788, ar 10 175.4±147.4, 100 185.3±128.5, ex 100 0.3±0.1, ev 460.9±066.3\n",
      "\u001b[2Kel 10:27, ep 2583, ts 265742, ar 10 140.8±077.8, 100 184.1±124.8, ex 100 0.3±0.1, ev 457.6±069.7\n",
      "\u001b[2Kel 10:47, ep 2625, ts 274042, ar 10 235.8±150.1, 100 184.0±126.1, ex 100 0.3±0.1, ev 434.3±078.3\n",
      "\u001b[2Kel 11:07, ep 2665, ts 281413, ar 10 153.6±088.2, 100 181.1±119.0, ex 100 0.3±0.1, ev 430.5±079.2\n",
      "\u001b[2Kel 11:28, ep 2703, ts 289834, ar 10 227.9±142.4, 100 202.4±126.0, ex 100 0.3±0.1, ev 441.7±079.9\n",
      "\u001b[2Kel 11:48, ep 2748, ts 296294, ar 10 110.3±071.9, 100 174.7±122.4, ex 100 0.3±0.1, ev 447.6±073.2\n",
      "\u001b[2Kel 12:08, ep 2790, ts 302908, ar 10 091.2±063.4, 100 157.5±122.6, ex 100 0.3±0.1, ev 446.2±082.7\n",
      "\u001b[2Kel 12:28, ep 2826, ts 310527, ar 10 239.1±124.9, 100 173.5±129.1, ex 100 0.3±0.1, ev 446.6±082.8\n",
      "\u001b[2Kel 12:49, ep 2867, ts 317610, ar 10 171.3±135.4, 100 176.4±135.8, ex 100 0.3±0.1, ev 452.8±080.1\n",
      "\u001b[2Kel 13:01, ep 2891, ts 321557, ar 10 118.4±093.1, 100 185.5±133.9, ex 100 0.3±0.1, ev 475.3±051.2\n",
      "--> reached_goal_mean_reward ✓\n",
      "Training complete.\n",
      "Final evaluation score 469.89±42.01 in 297.59s training time, 816.73s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 010.0±000.0\n",
      "\u001b[2Kel 00:20, ep 0504, ts 010607, ar 10 049.4±030.7, 100 039.9±023.7, ex 100 0.3±0.1, ev 047.5±026.4\n",
      "\u001b[2Kel 00:40, ep 0647, ts 020833, ar 10 076.0±046.8, 100 076.2±043.5, ex 100 0.3±0.1, ev 121.7±079.3\n",
      "\u001b[2Kel 01:00, ep 0806, ts 029814, ar 10 094.2±052.5, 100 032.6±043.5, ex 100 0.2±0.1, ev 049.7±084.6\n",
      "el 01:07, ep 0852, ts 032744, ar 10 079.5±055.9, 100 046.1±061.3, ex 100 0.2±0.1, ev 096.7±092.5\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ee0a16b22347>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmake_env_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_env_kargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_make_env_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     result, final_eval_score, training_time, wallclock_time = agent.train(\n\u001b[1;32m---> 38\u001b[1;33m         make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mnfq_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mnfq_agents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ea81a71fff16>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_seconds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_elapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mtraining_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mepisode_elapsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mevaluation_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0mtotal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_timestep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ea81a71fff16>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_policy_model, eval_env, n_episodes)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-b026d29df520>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, model, state)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nfq_results = []\n",
    "nfq_agents, best_nfq_agent_key, best_eval_score = {}, None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'CartPole-v1',\n",
    "        'gamma': 1.00,\n",
    "        'max_minutes': 20,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 475\n",
    "    }\n",
    "    \n",
    "    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n",
    "    # value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "\n",
    "    training_strategy_fn = lambda: EGreedyStrategy(epsilon=0.5)\n",
    "    # evaluation_strategy_fn = lambda: EGreedyStrategy(epsilon=0.05)\n",
    "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "\n",
    "    batch_size = 1024\n",
    "    epochs = 40\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = NFQ(value_model_fn, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr,\n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                batch_size,\n",
    "                epochs)\n",
    "\n",
    "    # make_env_fn, make_env_kargs = get_make_env_fn(\n",
    "    #     env_name=env_name, addon_wrappers=[DiscountedCartPole,])\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    nfq_results.append(result)\n",
    "    nfq_agents[seed] = agent\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_nfq_agent_key = seed\n",
    "nfq_results = np.array(nfq_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfq_agents[best_nfq_agent_key].demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfq_max_t, nfq_max_r, nfq_max_s, \\\n",
    "    nfq_max_sec, nfq_max_rt = np.max(nfq_results, axis=0).T\n",
    "nfq_min_t, nfq_min_r, nfq_min_s, \\\n",
    "    nfq_min_sec, nfq_min_rt = np.min(nfq_results, axis=0).T\n",
    "nfq_mean_t, nfq_mean_r, nfq_mean_s, \\\n",
    "    nfq_mean_sec, nfq_mean_rt = np.mean(nfq_results, axis=0).T\n",
    "nfq_x = np.arange(len(nfq_mean_s))\n",
    "\n",
    "# nfq_max_t, nfq_max_r, nfq_max_s, \\\n",
    "#     nfq_max_sec, nfq_max_rt = np.nanmax(nfq_results, axis=0).T\n",
    "# nfq_min_t, nfq_min_r, nfq_min_s, \\\n",
    "#     nfq_min_sec, nfq_min_rt = np.nanmin(nfq_results, axis=0).T\n",
    "# nfq_mean_t, nfq_mean_r, nfq_mean_s, \\\n",
    "#     nfq_mean_sec, nfq_mean_rt = np.nanmean(nfq_results, axis=0).T\n",
    "# nfq_x = np.arange(len(nfq_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n",
    "\n",
    "# NFQ\n",
    "axs[0].plot(nfq_max_r, 'y', linewidth=1)\n",
    "axs[0].plot(nfq_min_r, 'y', linewidth=1)\n",
    "axs[0].plot(nfq_mean_r, 'y', label='NFQ', linewidth=2)\n",
    "axs[0].fill_between(nfq_x, nfq_min_r, nfq_max_r, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[1].plot(nfq_max_s, 'y', linewidth=1)\n",
    "axs[1].plot(nfq_min_s, 'y', linewidth=1)\n",
    "axs[1].plot(nfq_mean_s, 'y', label='NFQ', linewidth=2)\n",
    "axs[1].fill_between(nfq_x, nfq_min_s, nfq_max_s, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[2].plot(nfq_max_t, 'y', linewidth=1)\n",
    "axs[2].plot(nfq_min_t, 'y', linewidth=1)\n",
    "axs[2].plot(nfq_mean_t, 'y', label='NFQ', linewidth=2)\n",
    "axs[2].fill_between(nfq_x, nfq_min_t, nfq_max_t, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[3].plot(nfq_max_sec, 'y', linewidth=1)\n",
    "axs[3].plot(nfq_min_sec, 'y', linewidth=1)\n",
    "axs[3].plot(nfq_mean_sec, 'y', label='NFQ', linewidth=2)\n",
    "axs[3].fill_between(nfq_x, nfq_min_sec, nfq_max_sec, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[4].plot(nfq_max_rt, 'y', linewidth=1)\n",
    "axs[4].plot(nfq_min_rt, 'y', linewidth=1)\n",
    "axs[4].plot(nfq_mean_rt, 'y', label='NFQ', linewidth=2)\n",
    "axs[4].fill_between(nfq_x, nfq_min_rt, nfq_max_rt, facecolor='y', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "axs[2].set_title('Total Steps')\n",
    "axs[3].set_title('Training Time')\n",
    "axs[4].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfq_root_dir = os.path.join(RESULTS_DIR, 'nfq')\n",
    "not os.path.exists(nfq_root_dir) and os.makedirs(nfq_root_dir)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'x'), nfq_x)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'max_r'), nfq_max_r)\n",
    "np.save(os.path.join(nfq_root_dir, 'min_r'), nfq_min_r)\n",
    "np.save(os.path.join(nfq_root_dir, 'mean_r'), nfq_mean_r)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'max_s'), nfq_max_s)\n",
    "np.save(os.path.join(nfq_root_dir, 'min_s'), nfq_min_s )\n",
    "np.save(os.path.join(nfq_root_dir, 'mean_s'), nfq_mean_s)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'max_t'), nfq_max_t)\n",
    "np.save(os.path.join(nfq_root_dir, 'min_t'), nfq_min_t)\n",
    "np.save(os.path.join(nfq_root_dir, 'mean_t'), nfq_mean_t)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'max_sec'), nfq_max_sec)\n",
    "np.save(os.path.join(nfq_root_dir, 'min_sec'), nfq_min_sec)\n",
    "np.save(os.path.join(nfq_root_dir, 'mean_sec'), nfq_mean_sec)\n",
    "\n",
    "np.save(os.path.join(nfq_root_dir, 'max_rt'), nfq_max_rt)\n",
    "np.save(os.path.join(nfq_root_dir, 'min_rt'), nfq_min_rt)\n",
    "np.save(os.path.join(nfq_root_dir, 'mean_rt'), nfq_mean_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
